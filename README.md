# Bank-Telemarketing-Success-Prediction
We would like to predict if the customer will respond positively to the product or services when they receive the marketing campaign, to increase response rate and ROI

Business Understanding
Commercial Banks provide financial services from deposits, checking accounts to personal loans and mortgages to individual consumers and small businesses. They often have direct marketing campaigns to acquire new customers or cross-sell additional products and services to existing customers (dmnnews.com, 2009). These direct marketing efforts have evolved over time, ranging from in-branch communications, direct mails, to emails and social media. With a growing number of communication channels and a saturated marketplace, commercial banks are facing the challenge of declining response rates from these direct marketing efforts (The Financial Brand, 2016). 
We would like to use machine learning to help commercial banks achieve better direct marketing results. We want to come up with a model that uses the customer’s demographics information, previous interactions and economical context to predict if the customer will respond positively to the product or services being promoted in the campaign. With this model, commercial banks can have a better understanding of the characteristics that lead to a positive response to the marketing efforts. They can also rank potential customers by the likelihood to respond so that they can determine which customers to target based on marketing budget, leading to a better return on investments for these direct marketing efforts.

Modeling
To begin our modelling process, we first used a standard scaler in order to fit and transform our X train and test sets. After this we defined an inner and outer cross validation as a KFold where n equals five and the random state is fourty two.
1.	Artificial Neural Network 
We implemented an advanced deep learning Algorithm using keras, which is capable of running on top of TensorFlow. Designed to enable fast experimentation with deep neural networks, it focuses on being user-friendly, modular, and extensible. Our Data preparation for this model included label encoding and one hot encoding for categorical variables and standardization for the train, validation and test datasets (excluding target variable). We customized the layers in this sequential classifier as follows : number of neurons in the input layer is equivalent to number of variables in the train dataset (Having a value of 56). We experimented depth and width for hidden layers and finally figured that 2 hidden layers with 10 neurons in the first and 8 neurons in the second is the most optimal. Anything beyond that would overfit the data while anything below would underfit it. Activation function used is “relu” for hidden layers. Our output layer here has only one neuron since we are looking at a binary classification problem. Acitvation function used for output layer is sigmoid. We then further compiled the neural network with an optimizer : An “adam” ,loss function (which is  a “binary cross entropy”) and a f1 score as the metric.  We used a batch size as 20 such that weights are updated after 20 inputs from the train data with back propagation. We chose the epochs to be a 100 and noticed that after 25 epochs the result is converges. Finally we applied the model on the test dataset. 
2.	Decision Tree
A popular data science model, decision trees are a supervised learning method used for regressions and classifications. Following a flowchart-like structure, decision trees predict the value of a target variable by learning basic decision rules. We began by using a Grid search Cross Validation technique with a random state of 42 in order to find the highest accuracy. This technique outputted the ‘best’ values for the parameters fed which were: ‘max_depth’ = 8, ‘max_leaf_nodes’ = 20 and ‘mini_samples_split’ = 2. 
3.	K-Nearest Neighbors (KNN) 
Another algorithm that we implemented was K-NN. Being a non-parametric algorithm, we used it to see if computing distances and obtaining k-nearest data samples would give us a high accuracy value. Similar to the Decision Tree process, we used a  GridSearch Cross Validations technique. By defining our X and Y variables, and using n_splits = 5, we were able to tune the necessary parameters. Based on the parameters we fed the system, it outputted metric = ‘minkowski’,  n_neighbors = 21, weights = ‘distance and a leaf size of 30. 
4.	Logistic Regression
Logistic Regression is another famous statistical model used for classification problems and is based on probability. We began by using a Grid search Cross Validation technique with a random seed of 42 in order to find the highest accuracy. This technique outputted the ‘best’ values for the parameters inputted which were: ‘penalty’ = l2, ‘solver = ‘lbfgs‘ and ‘max_iter’ = 100.
5.	Naïve Bayes 
A Naive Bayes classifier is a probabilistic machine learning model that is used for classification task. The crux of the classifier is based on the Bayes theorem. We did not apply grid search cv on Naïve Bayes as there weren’t any optimal parameters to be experimented on.  Instead we trained the model on train dataset and tested the model on the test dataset in order to see its  performance.
6.	Random Forest 
Random Forest is an ensemble tree-based learning algorithm that uses a set of decision trees from a randomly selected subset of a training set. It aggregates the votes from different decision trees in order to decide the final class of the test object. We hyper tuned the parameters such as n_estimators with values such as 50,100,200 ,max_depth with values such as 1,3,5, 10, 20, 50, min_impurity decrease with values such as 0.1, 0.01,0.001 and finally max_features with values such as 'sqrt' and 'log2'. We found that the best hyper parameters are max_depth =10 ,max_features=none, min_impurity_decrease=0.001, n_estimators =100
7.	Ensemble Learning
Apart from the algorithms mentioned above, we also applied ensemble learning models – XGBoost and LightGBM - to further improve the outcomes. 
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework and provides a parallel tree boosting that solves many data science problems in a fast and accurate way; LightGBM is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with the advantages of higher efficiency, lower memory usage and better accuracy.
First, we tuned the parameters of XGBoost and LightGBM. For XGBoost, we focused on three important parameters, ‘learning_rate’, ‘max_depth’, and ‘min_child_weight’, with F1 score to make the parameter selections. Learning rate usually fluctuates from 0.0 to 0.5. Max depth and minimal child weight can range from 1 to 10. We then applied these ranges on cross-validation grid search to find optimal parameters: ‘learning_rate’ = 0.45, ‘max_depth’ = 10, and ‘min_child_weight’ = 1.
For LightGBM, we focused on three important parameters, ‘learning_rate’, ‘max_depth’, and ‘num_leaves’, with F1 score to make the parameter selections. Learning rate usually fluctuates from 0.0 to 0.5. Max depth can range from 4 to 13. Number of leaves can range from 20 to 170. We then applied these ranges on cross-validation grid search to find optimal parameters: ‘learning_rate’ = 0.45, ‘max_depth’ = 11, and ‘num_leaves’ = 140.
To obtain the possibility of “yes” on the test data and rank them accordingly, we applied LightGBM regressors. We also tuned three parameters: ‘learning_rate’ = 0.25, ‘max_depth’ = 13, and ‘num_leaves’ = 140. 
We utilized tuned parameters on the algorithms to train the model using training set and applied it on the test set to evaluate generalization performance.

Choosing the best model
In our business context, we have different costs and benefits. Our cost is huge for falsely predicting that a customer wouldn’t subscribe a term deposit whereas that customer in reality is a potential customer. And our cost is not so huge for falsely predicting that a customer would subscribe a term deposit where as that customer in reality is not a potential customer because sending out promotions is not expensive at all. The medium of promotions is just a phone call. All it costs is some time from an executive but barely any monetary cost. So it’s imperative that our model has minimal false negative rate or higher positive precision. Looking at the above table, we see that boosting algorithms are performing significantly better than any other machine learning algorithms. So we could reject those machine learning algorithms straightaway. For the remaining two boosting algorithms, the performance is fairly similar in terms of every metric. Therefore, we can conclude that LightGBM is the best model, based on optimal performance and computation efficiency.

Deployment
Based on the modeling results, we recommend that banks use the LightGBM to predict if customers will respond positively to the term deposit telemarketing campaign. With this model, banks can generate a response rate increase of 22.6% (comparing the test data response rate of 98.1% to the population response rate of 80%), while using 77% of the original marketing expenses. This allows the banks to cut down on marketing expenses when necessary, and at the same time, have a higher response rate and return on investments with these telemarketing campaigns on term deposits. The LightGBM model can also be deployed to generate probability predictions, so that banks can rank customers by probability to respond positively, and target customers with high probability to respond based on their limited marketing budget. 
However, the model also has some limitations in deployment. Each telemarketing campaign is slightly different in the product that it promotes and the audience that it appeals to, so it is important to reassess whenever the telemarketing campaign product changes. The communication channel also plays a considerate part in influencing customer decisions, so reassessment is also needed if the communication channel changes from telemarketing to direct mails or etc. With these limitation in deployment scopes, we hope to expand the data set to include data collected from more marketing campaigns and channels, to explore if we can generalize the model to accommodate more marketing types and channels. The data set we used also contains a lot of unknowns. We would like to improve the completeness of the data to have better performance.
 
Bibliography
TODÉ, CHANTAL. “Banks Increase Direct Marketing to Customers.” DMNews.com, 2009, www.dmnews.com/marketing-channels/direct-mail/news/13063167/banks-increase-direct-marketing-to-customers.
Pilcher, Jeffry. “How Banks Can Get Results With Direct Marketing In The Digital Age.” The Financial Brand, 1 Dec. 2016, thefinancialbrand.com/25477/digital-direct-marketing-in-banking/.
“UCI Machine Learning Repository: Bank Marketing Data Set.” UCI Machine Learning Repository: Bank Marketing Data Set, 2014, archive.ics.uci.edu/ml/datasets/Bank+Marketing.
S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014
